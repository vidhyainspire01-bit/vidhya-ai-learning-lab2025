{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e381e929-8cbf-4e0e-a4f9-2e566b556198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Accuracy by Model (k=6, SelectKBest chi2) ===\n",
      "           Model  Accuracy\n",
      "0   SVM (Linear)      0.93\n",
      "1      SVM (RBF)      0.93\n",
      "2            KNN      0.93\n",
      "3  Decision Tree      0.93\n",
      "4    Naive Bayes      0.93\n",
      "5  Random Forest      0.93\n",
      "6       Logistic      0.92\n",
      "\n",
      "=== Classification Report: SVM (Linear) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.97      0.91        38\n",
      "        True       0.98      0.90      0.94        62\n",
      "\n",
      "    accuracy                           0.93       100\n",
      "   macro avg       0.92      0.94      0.93       100\n",
      "weighted avg       0.94      0.93      0.93       100\n",
      "\n",
      "=== Confusion Matrix: SVM (Linear) ===\n",
      "[[37  1]\n",
      " [ 6 56]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ---------- Load & prepare data ----------\n",
    "df = pd.read_csv(\"prep.csv\")\n",
    "\n",
    "# One-hot encode; drop_first to avoid dummy trap\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Target column (adjust if your positive class column differs)\n",
    "target_col = \"classification_yes\"\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Train/test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "# ---------- Helper to build a pipeline ----------\n",
    "# chi2 needs non-negative inputs -> MinMax BEFORE SelectKBest\n",
    "# Some models like SVM/KNN/LogReg benefit from standardization after feature selection.\n",
    "def make_pipeline(clf, k=6, standardize_after=True):\n",
    "    steps = [\n",
    "        (\"minmax\", MinMaxScaler()),\n",
    "        (\"kbest\", SelectKBest(score_func=chi2, k=k)),\n",
    "    ]\n",
    "    if standardize_after:\n",
    "        steps.append((\"std\", StandardScaler()))\n",
    "    steps.append((\"clf\", clf))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "# Pipelines per model\n",
    "pipelines = {\n",
    "    \"Logistic\": make_pipeline(LogisticRegression(max_iter=5000, solver=\"liblinear\")),\n",
    "    \"SVM (Linear)\": make_pipeline(SVC(kernel=\"linear\", probability=False, random_state=0)),\n",
    "    \"SVM (RBF)\": make_pipeline(SVC(kernel=\"rbf\", probability=False, random_state=0)),\n",
    "    \"KNN\": make_pipeline(KNeighborsClassifier(n_neighbors=5)),\n",
    "    # GaussianNB assumes features ~ Gaussian; standardization ok\n",
    "    \"Naive Bayes\": make_pipeline(GaussianNB()),\n",
    "    # Tree/Forest don’t need standardization; set standardize_after=False\n",
    "    \"Decision Tree\": make_pipeline(\n",
    "        DecisionTreeClassifier(criterion=\"entropy\", random_state=0),\n",
    "        standardize_after=False\n",
    "    ),\n",
    "    \"Random Forest\": make_pipeline(\n",
    "        RandomForestClassifier(n_estimators=200, criterion=\"entropy\", random_state=0),\n",
    "        standardize_after=False\n",
    "    ),\n",
    "}\n",
    "\n",
    "# ---------- Train, evaluate, collect metrics ----------\n",
    "rows = []\n",
    "reports = {}\n",
    "cms = {}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    rpt = classification_report(y_test, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    rows.append({\"Model\": name, \"Accuracy\": acc})\n",
    "    reports[name] = rpt\n",
    "    cms[name] = cm\n",
    "\n",
    "results = pd.DataFrame(rows).sort_values(\"Accuracy\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== Accuracy by Model (k=6, SelectKBest chi2) ===\")\n",
    "print(results)\n",
    "\n",
    "# If you want to see one model’s detailed report & confusion matrix:\n",
    "best_model = results.iloc[0][\"Model\"]\n",
    "print(f\"\\n=== Classification Report: {best_model} ===\\n{reports[best_model]}\")\n",
    "print(f\"=== Confusion Matrix: {best_model} ===\\n{cms[best_model]}\")\n",
    "\n",
    "# ---------- Optional: check which k works best ----------\n",
    "# Try multiple k values and record the best accuracy per model\n",
    "def benchmark_k(k_values=(3,5,10,15,20)):\n",
    "    out = []\n",
    "    for k in k_values:\n",
    "        for name, _ in pipelines.items():\n",
    "            # Rebuild each pipeline with this k\n",
    "            standardize_after = name not in [\"Decision Tree\", \"Random Forest\"]\n",
    "            clf = {\n",
    "                \"Logistic\": LogisticRegression(max_iter=5000, solver=\"liblinear\"),\n",
    "                \"SVM (Linear)\": SVC(kernel=\"linear\", random_state=0),\n",
    "                \"SVM (RBF)\": SVC(kernel=\"rbf\", random_state=0),\n",
    "                \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "                \"Naive Bayes\": GaussianNB(),\n",
    "                \"Decision Tree\": DecisionTreeClassifier(criterion=\"entropy\", random_state=0),\n",
    "                \"Random Forest\": RandomForestClassifier(n_estimators=200, criterion=\"entropy\", random_state=0),\n",
    "            }[name]\n",
    "            pipe = make_pipeline(clf, k=k, standardize_after=standardize_after)\n",
    "            pipe.fit(X_train, y_train)\n",
    "            acc = accuracy_score(y_test, pipe.predict(X_test))\n",
    "            out.append({\"k\": k, \"Model\": name, \"Accuracy\": acc})\n",
    "    return pd.DataFrame(out).sort_values([\"Model\", \"Accuracy\"], ascending=[True, False])\n",
    "\n",
    "# Example (uncomment to run):\n",
    "# k_scan = benchmark_k()\n",
    "# print(\"\\n=== Accuracy vs k (top rows) ===\")\n",
    "# print(k_scan.groupby(\"Model\").head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24e00380-058f-46b0-bc64-dd9f17c71d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Logistic  SVMl  SVMnl   KNN  Navie  Decision  Random\n",
      "ChiSquare      0.93  0.94   0.93  0.92   0.94      0.94    0.94\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# --- load & prep ---\n",
    "df = pd.read_csv(\"prep.csv\")\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "y = df[\"classification_yes\"]\n",
    "X = df.drop(columns=[\"classification_yes\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "k = 5  # match your run\n",
    "\n",
    "def mkpipe(clf, std_after=True, n_estimators=None):\n",
    "    steps = [(\"mm\", MinMaxScaler()), (\"kbest\", SelectKBest(chi2, k=k))]\n",
    "    if std_after:\n",
    "        steps.append((\"std\", StandardScaler()))\n",
    "    steps.append((\"clf\", clf))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "pipes = {\n",
    "    \"Logistic\": mkpipe(LogisticRegression(max_iter=5000, solver=\"liblinear\")),\n",
    "    \"SVMl\": mkpipe(SVC(kernel=\"linear\", random_state=0)),\n",
    "    \"SVMnl\": mkpipe(SVC(kernel=\"rbf\", random_state=0)),\n",
    "    \"KNN\": mkpipe(KNeighborsClassifier(n_neighbors=5)),\n",
    "    \"Navie\": mkpipe(GaussianNB()),\n",
    "    \"Decision\": mkpipe(DecisionTreeClassifier(criterion=\"entropy\", random_state=0), std_after=False),\n",
    "    \"Random\": mkpipe(RandomForestClassifier(n_estimators=10, criterion=\"entropy\", random_state=0), std_after=False),\n",
    "}\n",
    "\n",
    "accs = {}\n",
    "for name, pipe in pipes.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    accs[name] = accuracy_score(y_test, pipe.predict(X_test))\n",
    "\n",
    "# format like your one-row table\n",
    "result = pd.DataFrame([accs], index=[\"ChiSquare\"])[[\"Logistic\",\"SVMl\",\"SVMnl\",\"KNN\",\"Navie\",\"Decision\",\"Random\"]]\n",
    "print(result.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc01cb3a-5018-4941-99f9-0e51132002ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Logistic  SVMl  SVMnl   KNN  Navie  Decision  Random\n",
      "ChiSquare      0.95  0.96   0.96  0.96   0.96      0.96    0.96\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# --- load & prep ---\n",
    "df = pd.read_csv(\"prep.csv\")\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "y = df[\"classification_yes\"]\n",
    "X = df.drop(columns=[\"classification_yes\"])\n",
    "\n",
    "#stratify=y ensures class balance is preserved in both sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "k = 7 # match your run\n",
    "\n",
    "def mkpipe(clf, std_after=True, n_estimators=None):\n",
    "    steps = [(\"mm\", MinMaxScaler()), (\"kbest\", SelectKBest(chi2, k=k))]\n",
    "    if std_after:\n",
    "        steps.append((\"std\", StandardScaler()))\n",
    "    steps.append((\"clf\", clf))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "pipes = {\n",
    "    \"Logistic\": mkpipe(LogisticRegression(max_iter=5000, solver=\"liblinear\")),\n",
    "    \"SVMl\": mkpipe(SVC(kernel=\"linear\", random_state=0)),\n",
    "    \"SVMnl\": mkpipe(SVC(kernel=\"rbf\", random_state=0)),\n",
    "    \"KNN\": mkpipe(KNeighborsClassifier(n_neighbors=5)),\n",
    "    \"Navie\": mkpipe(GaussianNB()),\n",
    "    \"Decision\": mkpipe(DecisionTreeClassifier(criterion=\"entropy\", random_state=0), std_after=False),\n",
    "    \"Random\": mkpipe(RandomForestClassifier(n_estimators=10, criterion=\"entropy\", random_state=0), std_after=False),\n",
    "}\n",
    "\n",
    "accs = {}\n",
    "for name, pipe in pipes.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    accs[name] = accuracy_score(y_test, pipe.predict(X_test))\n",
    "\n",
    "# format like your one-row table\n",
    "result = pd.DataFrame([accs], index=[\"ChiSquare\"])[[\"Logistic\",\"SVMl\",\"SVMnl\",\"KNN\",\"Navie\",\"Decision\",\"Random\"]]\n",
    "print(result.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07875b91-dcfa-4fe1-8df4-5ca7f2680f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Logistic  SVMl  SVMnl   KNN  Navie  Decision  Random\n",
      "ChiSquare      0.96  0.96   0.96  0.96   0.96      0.96    0.96\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# --- load & prep ---\n",
    "df = pd.read_csv(\"prep.csv\")\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "y = df[\"classification_yes\"]\n",
    "X = df.drop(columns=[\"classification_yes\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "k = 8 # match your run\n",
    "\n",
    "def mkpipe(clf, std_after=True, n_estimators=None):\n",
    "    steps = [(\"mm\", MinMaxScaler()), (\"kbest\", SelectKBest(chi2, k=k))]\n",
    "    if std_after:\n",
    "        steps.append((\"std\", StandardScaler()))\n",
    "    steps.append((\"clf\", clf))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "pipes = {\n",
    "    \"Logistic\": mkpipe(LogisticRegression(max_iter=5000, solver=\"liblinear\")),\n",
    "    \"SVMl\": mkpipe(SVC(kernel=\"linear\", random_state=0)),\n",
    "    \"SVMnl\": mkpipe(SVC(kernel=\"rbf\", random_state=0)),\n",
    "    \"KNN\": mkpipe(KNeighborsClassifier(n_neighbors=5)),\n",
    "    \"Navie\": mkpipe(GaussianNB()),\n",
    "    \"Decision\": mkpipe(DecisionTreeClassifier(criterion=\"entropy\", random_state=0), std_after=False),\n",
    "    \"Random\": mkpipe(RandomForestClassifier(n_estimators=10, criterion=\"entropy\", random_state=0), std_after=False),\n",
    "}\n",
    "\n",
    "accs = {}\n",
    "for name, pipe in pipes.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    accs[name] = accuracy_score(y_test, pipe.predict(X_test))\n",
    "\n",
    "# format like your one-row table\n",
    "result = pd.DataFrame([accs], index=[\"ChiSquare\"])[[\"Logistic\",\"SVMl\",\"SVMnl\",\"KNN\",\"Navie\",\"Decision\",\"Random\"]]\n",
    "print(result.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89591cf2-9304-4e3c-b0d6-c062aac0db5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features selected by Chi²:\n",
      "['al', 'sg_b', 'sg_c', 'sg_d', 'htn_yes', 'dm_yes', 'pe_yes', 'ane_yes']\n",
      "\n",
      "Model Accuracies:\n",
      "           Logistic  SVMl  SVMnl   KNN  Naive  Decision  Random\n",
      "ChiSquare      0.96  0.96   0.96  0.96   0.96      0.96    0.96\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# --- load & prep ---\n",
    "df = pd.read_csv(\"prep.csv\")\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "y = df[\"classification_yes\"]\n",
    "X = df.drop(columns=[\"classification_yes\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "k = 8  # number of top features to select\n",
    "\n",
    "def mkpipe(clf, std_after=True):\n",
    "    steps = [\n",
    "        (\"mm\", MinMaxScaler()),\n",
    "        (\"kbest\", SelectKBest(chi2, k=k))\n",
    "    ]\n",
    "    if std_after:\n",
    "        steps.append((\"std\", StandardScaler()))\n",
    "    steps.append((\"clf\", clf))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "# --- example: Logistic Regression ---\n",
    "pipe = mkpipe(LogisticRegression(max_iter=5000, solver=\"liblinear\"))\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Get selected features\n",
    "selector = pipe.named_steps[\"kbest\"]\n",
    "selected_mask = selector.get_support()\n",
    "selected_features = X.columns[selected_mask]\n",
    "\n",
    "print(\"Top features selected by Chi²:\")\n",
    "print(selected_features.tolist())\n",
    "\n",
    "# --- evaluate models ---\n",
    "pipes = {\n",
    "    \"Logistic\": mkpipe(LogisticRegression(max_iter=5000, solver=\"liblinear\")),\n",
    "    \"SVMl\": mkpipe(SVC(kernel=\"linear\", random_state=0)),\n",
    "    \"SVMnl\": mkpipe(SVC(kernel=\"rbf\", random_state=0)),\n",
    "    \"KNN\": mkpipe(KNeighborsClassifier(n_neighbors=5)),\n",
    "    \"Naive\": mkpipe(GaussianNB()),\n",
    "    \"Decision\": mkpipe(DecisionTreeClassifier(criterion=\"entropy\", random_state=0), std_after=False),\n",
    "    \"Random\": mkpipe(RandomForestClassifier(n_estimators=10, criterion=\"entropy\", random_state=0), std_after=False),\n",
    "}\n",
    "\n",
    "accs = {}\n",
    "for name, pipe in pipes.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    accs[name] = accuracy_score(y_test, pipe.predict(X_test))\n",
    "\n",
    "result = pd.DataFrame([accs], index=[\"ChiSquare\"])[[\"Logistic\",\"SVMl\",\"SVMnl\",\"KNN\",\"Naive\",\"Decision\",\"Random\"]]\n",
    "print(\"\\nModel Accuracies:\")\n",
    "print(result.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "485053d5-2384-40e4-95f5-27a40cec0bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bp</th>\n",
       "      <th>al</th>\n",
       "      <th>su</th>\n",
       "      <th>bgr</th>\n",
       "      <th>bu</th>\n",
       "      <th>sc</th>\n",
       "      <th>sod</th>\n",
       "      <th>pot</th>\n",
       "      <th>hrmo</th>\n",
       "      <th>...</th>\n",
       "      <th>pc_normal</th>\n",
       "      <th>pcc_present</th>\n",
       "      <th>ba_present</th>\n",
       "      <th>htn_yes</th>\n",
       "      <th>dm_yes</th>\n",
       "      <th>cad_yes</th>\n",
       "      <th>appet_yes</th>\n",
       "      <th>pe_yes</th>\n",
       "      <th>ane_yes</th>\n",
       "      <th>classification_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>76.459948</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>148.112676</td>\n",
       "      <td>57.482105</td>\n",
       "      <td>3.077356</td>\n",
       "      <td>137.528754</td>\n",
       "      <td>4.627244</td>\n",
       "      <td>12.518156</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>76.459948</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>148.112676</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>137.528754</td>\n",
       "      <td>4.627244</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>76.459948</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>76.459948</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>148.112676</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>148.112676</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>137.528754</td>\n",
       "      <td>4.627244</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>51.492308</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>51.492308</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>137.528754</td>\n",
       "      <td>4.627244</td>\n",
       "      <td>8.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>51.492308</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>51.492308</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>51.492308</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>399 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age         bp   al   su         bgr          bu        sc  \\\n",
       "0     2.000000  76.459948  3.0  0.0  148.112676   57.482105  3.077356   \n",
       "1     3.000000  76.459948  2.0  0.0  148.112676   22.000000  0.700000   \n",
       "2     4.000000  76.459948  1.0  0.0   99.000000   23.000000  0.600000   \n",
       "3     5.000000  76.459948  1.0  0.0  148.112676   16.000000  0.700000   \n",
       "4     5.000000  50.000000  0.0  0.0  148.112676   25.000000  0.600000   \n",
       "..         ...        ...  ...  ...         ...         ...       ...   \n",
       "394  51.492308  70.000000  0.0  0.0  219.000000   36.000000  1.300000   \n",
       "395  51.492308  70.000000  0.0  2.0  220.000000   68.000000  2.800000   \n",
       "396  51.492308  70.000000  3.0  0.0  110.000000  115.000000  6.000000   \n",
       "397  51.492308  90.000000  0.0  0.0  207.000000   80.000000  6.800000   \n",
       "398  51.492308  80.000000  0.0  0.0  100.000000   49.000000  1.000000   \n",
       "\n",
       "            sod       pot       hrmo  ...  pc_normal  pcc_present  ba_present  \\\n",
       "0    137.528754  4.627244  12.518156  ...      False        False       False   \n",
       "1    137.528754  4.627244  10.700000  ...       True        False       False   \n",
       "2    138.000000  4.400000  12.000000  ...       True        False       False   \n",
       "3    138.000000  3.200000   8.100000  ...       True        False       False   \n",
       "4    137.528754  4.627244  11.800000  ...       True        False       False   \n",
       "..          ...       ...        ...  ...        ...          ...         ...   \n",
       "394  139.000000  3.700000  12.500000  ...       True        False       False   \n",
       "395  137.528754  4.627244   8.700000  ...       True        False       False   \n",
       "396  134.000000  2.700000   9.100000  ...       True        False       False   \n",
       "397  142.000000  5.500000   8.500000  ...       True        False       False   \n",
       "398  140.000000  5.000000  16.300000  ...       True        False       False   \n",
       "\n",
       "     htn_yes  dm_yes  cad_yes  appet_yes  pe_yes  ane_yes  classification_yes  \n",
       "0      False   False    False       True    True    False                True  \n",
       "1      False   False    False       True   False    False                True  \n",
       "2      False   False    False       True   False    False                True  \n",
       "3      False   False    False       True   False     True                True  \n",
       "4      False   False    False       True   False    False                True  \n",
       "..       ...     ...      ...        ...     ...      ...                 ...  \n",
       "394    False   False    False       True   False    False                True  \n",
       "395     True    True    False       True   False     True                True  \n",
       "396     True    True    False      False   False    False                True  \n",
       "397     True    True    False       True   False     True                True  \n",
       "398    False   False    False       True   False    False               False  \n",
       "\n",
       "[399 rows x 28 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901b5838-5981-4af7-a347-dc3dae65704b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic   -> best_k=18, cv_acc=0.9900\n",
      "SVMl       -> best_k=19, cv_acc=0.9933\n",
      "SVMnl      -> best_k=17, cv_acc=0.9933\n",
      "KNN        -> best_k=17, cv_acc=0.9866\n",
      "Navie      -> best_k=15, cv_acc=0.9900\n",
      "Decision   -> best_k=7, cv_acc=0.9799\n",
      "Random     -> best_k=20, cv_acc=0.9899\n",
      "\n",
      "Best k per model (by CV mean accuracy):\n",
      "      model   k  cv_mean_acc\n",
      "0  Decision   7     0.979944\n",
      "1       KNN  17     0.986610\n",
      "2  Logistic  18     0.990000\n",
      "3     Navie  15     0.990000\n",
      "4    Random  20     0.989944\n",
      "5      SVMl  19     0.993277\n",
      "6     SVMnl  17     0.993333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# --- load & prep ---\n",
    "df = pd.read_csv(\"prep.csv\")\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "y = df[\"classification_yes\"]\n",
    "X = df.drop(columns=[\"classification_yes\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# def mkpipe(clf, std_after=True, n_estimators=None):\n",
    "#     steps = [(\"mm\", MinMaxScaler()), (\"kbest\", SelectKBest(chi2, k=k))]\n",
    "#     if std_after:\n",
    "#         steps.append((\"std\", StandardScaler()))\n",
    "#     steps.append((\"clf\", clf))\n",
    "#     return Pipeline(steps)\n",
    "\n",
    "pipes = {\n",
    "    \"Logistic\": (LogisticRegression(max_iter=5000, solver=\"liblinear\"),True),\n",
    "    \"SVMl\": (SVC(kernel=\"linear\", random_state=0),True),\n",
    "    \"SVMnl\": (SVC(kernel=\"rbf\", random_state=0),True),\n",
    "    \"KNN\": (KNeighborsClassifier(n_neighbors=5),True),\n",
    "    \"Navie\": (GaussianNB(), True),\n",
    "    \"Decision\": (DecisionTreeClassifier(criterion=\"entropy\", random_state=0), False),\n",
    "    \"Random\": (RandomForestClassifier(n_estimators=10, criterion=\"entropy\", random_state=0), False),\n",
    "}\n",
    "max_k = min(30, X_train.shape[1])\n",
    "k_list = list(range(1, max_k + 1))\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "results = []\n",
    "for name,(clf,std_after) in pipes.items():\n",
    "    best_k = None\n",
    "    best_score = -np.inf\n",
    "    for k in k_list:\n",
    "        steps = [(\"mm\", MinMaxScaler()), (\"kbest\", SelectKBest(chi2, k=k))]\n",
    "        if std_after:\n",
    "            steps.append((\"std\", StandardScaler()))\n",
    "        steps.append((\"clf\", clf))\n",
    "        pipe = Pipeline(steps)\n",
    "\n",
    "        scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "        mean_score = scores.mean()\n",
    "\n",
    "        results.append({\"model\": name, \"k\": k, \"cv_mean_acc\": mean_score})\n",
    "\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_k = k\n",
    "\n",
    "    print(f\"{name:10s} -> best_k={best_k}, cv_acc={best_score:.4f}\")\n",
    "\n",
    "# convert to DataFrame for analysis\n",
    "res_df = pd.DataFrame(results)\n",
    "# Show best k per model\n",
    "best_per_model = res_df.loc[res_df.groupby(\"model\")[\"cv_mean_acc\"].idxmax()].sort_values(\"model\")\n",
    "print(\"\\nBest k per model (by CV mean accuracy):\")\n",
    "print(best_per_model[[\"model\",\"k\",\"cv_mean_acc\"]].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b3c82ff-0e06-45ee-9258-6b51dfcd2554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Model: Logistic (k=18)\n",
      "Test Accuracy: 0.9700\n",
      "Selected features: ['al', 'su', 'sc', 'hrmo', 'pcv', 'rc', 'sg_b', 'sg_c', 'sg_d', 'pc_normal', 'pcc_present', 'ba_present', 'htn_yes', 'dm_yes', 'cad_yes', 'appet_yes', 'pe_yes', 'ane_yes']\n",
      "Confusion Matrix:\n",
      " [[37  1]\n",
      " [ 2 60]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.97      0.96        38\n",
      "        True       0.98      0.97      0.98        62\n",
      "\n",
      "    accuracy                           0.97       100\n",
      "   macro avg       0.97      0.97      0.97       100\n",
      "weighted avg       0.97      0.97      0.97       100\n",
      "\n",
      "\n",
      "============================================================\n",
      "Model: SVMl (k=19)\n",
      "Test Accuracy: 0.9800\n",
      "Selected features: ['al', 'su', 'bgr', 'sc', 'hrmo', 'pcv', 'rc', 'sg_b', 'sg_c', 'sg_d', 'pc_normal', 'pcc_present', 'ba_present', 'htn_yes', 'dm_yes', 'cad_yes', 'appet_yes', 'pe_yes', 'ane_yes']\n",
      "Confusion Matrix:\n",
      " [[37  1]\n",
      " [ 1 61]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.97      0.97        38\n",
      "        True       0.98      0.98      0.98        62\n",
      "\n",
      "    accuracy                           0.98       100\n",
      "   macro avg       0.98      0.98      0.98       100\n",
      "weighted avg       0.98      0.98      0.98       100\n",
      "\n",
      "\n",
      "============================================================\n",
      "Model: SVMnl (k=17)\n",
      "Test Accuracy: 0.9800\n",
      "Selected features: ['al', 'su', 'hrmo', 'pcv', 'rc', 'sg_b', 'sg_c', 'sg_d', 'pc_normal', 'pcc_present', 'ba_present', 'htn_yes', 'dm_yes', 'cad_yes', 'appet_yes', 'pe_yes', 'ane_yes']\n",
      "Confusion Matrix:\n",
      " [[37  1]\n",
      " [ 1 61]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.97      0.97        38\n",
      "        True       0.98      0.98      0.98        62\n",
      "\n",
      "    accuracy                           0.98       100\n",
      "   macro avg       0.98      0.98      0.98       100\n",
      "weighted avg       0.98      0.98      0.98       100\n",
      "\n",
      "\n",
      "============================================================\n",
      "Model: KNN (k=17)\n",
      "Test Accuracy: 0.9700\n",
      "Selected features: ['al', 'su', 'hrmo', 'pcv', 'rc', 'sg_b', 'sg_c', 'sg_d', 'pc_normal', 'pcc_present', 'ba_present', 'htn_yes', 'dm_yes', 'cad_yes', 'appet_yes', 'pe_yes', 'ane_yes']\n",
      "Confusion Matrix:\n",
      " [[37  1]\n",
      " [ 2 60]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.97      0.96        38\n",
      "        True       0.98      0.97      0.98        62\n",
      "\n",
      "    accuracy                           0.97       100\n",
      "   macro avg       0.97      0.97      0.97       100\n",
      "weighted avg       0.97      0.97      0.97       100\n",
      "\n",
      "\n",
      "============================================================\n",
      "Model: Navie (k=15)\n",
      "Test Accuracy: 0.9700\n",
      "Selected features: ['al', 'su', 'hrmo', 'pcv', 'sg_b', 'sg_c', 'sg_d', 'pcc_present', 'ba_present', 'htn_yes', 'dm_yes', 'cad_yes', 'appet_yes', 'pe_yes', 'ane_yes']\n",
      "Confusion Matrix:\n",
      " [[37  1]\n",
      " [ 2 60]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.97      0.96        38\n",
      "        True       0.98      0.97      0.98        62\n",
      "\n",
      "    accuracy                           0.97       100\n",
      "   macro avg       0.97      0.97      0.97       100\n",
      "weighted avg       0.97      0.97      0.97       100\n",
      "\n",
      "\n",
      "============================================================\n",
      "Model: Decision (k=7)\n",
      "Test Accuracy: 0.9600\n",
      "Selected features: ['al', 'sg_b', 'sg_c', 'sg_d', 'htn_yes', 'dm_yes', 'pe_yes']\n",
      "Confusion Matrix:\n",
      " [[37  1]\n",
      " [ 3 59]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.97      0.95        38\n",
      "        True       0.98      0.95      0.97        62\n",
      "\n",
      "    accuracy                           0.96       100\n",
      "   macro avg       0.95      0.96      0.96       100\n",
      "weighted avg       0.96      0.96      0.96       100\n",
      "\n",
      "\n",
      "============================================================\n",
      "Model: Random (k=20)\n",
      "Test Accuracy: 0.9800\n",
      "Selected features: ['al', 'su', 'bgr', 'bu', 'sc', 'hrmo', 'pcv', 'rc', 'sg_b', 'sg_c', 'sg_d', 'pc_normal', 'pcc_present', 'ba_present', 'htn_yes', 'dm_yes', 'cad_yes', 'appet_yes', 'pe_yes', 'ane_yes']\n",
      "Confusion Matrix:\n",
      " [[37  1]\n",
      " [ 1 61]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.97      0.97        38\n",
      "        True       0.98      0.98      0.98        62\n",
      "\n",
      "    accuracy                           0.98       100\n",
      "   macro avg       0.98      0.98      0.98       100\n",
      "weighted avg       0.98      0.98      0.98       100\n",
      "\n",
      "\n",
      "Summary of test accuracies:\n",
      "      model   k  test_acc\n",
      "0      SVMl  19      0.98\n",
      "1     SVMnl  17      0.98\n",
      "2    Random  20      0.98\n",
      "3  Logistic  18      0.97\n",
      "4       KNN  17      0.97\n",
      "5     Navie  15      0.97\n",
      "6  Decision   7      0.96\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# --- load & prep ---\n",
    "df = pd.read_csv(\"prep.csv\")\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "y = df[\"classification_yes\"]\n",
    "X = df.drop(columns=[\"classification_yes\"])\n",
    "\n",
    "# Split once (we used this earlier for CV)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "# Best k per your CV results:\n",
    "best_k_per_model = {\n",
    "    \"Logistic\": 18,\n",
    "    \"SVMl\": 19,\n",
    "    \"SVMnl\": 17,\n",
    "    \"KNN\": 17,\n",
    "    \"Navie\": 15,\n",
    "    \"Decision\": 7,\n",
    "    \"Random\": 20\n",
    "}\n",
    "\n",
    "# model definitions and whether to standardize after SelectKBest\n",
    "model_defs = {\n",
    "    \"Logistic\": (LogisticRegression(max_iter=5000, solver=\"liblinear\"), True),\n",
    "    \"SVMl\": (SVC(kernel=\"linear\", probability=False, random_state=0), True),\n",
    "    \"SVMnl\": (SVC(kernel=\"rbf\", probability=False, random_state=0), True),\n",
    "    \"KNN\": (KNeighborsClassifier(n_neighbors=5), True),\n",
    "    \"Navie\": (GaussianNB(), True),\n",
    "    \"Decision\": (DecisionTreeClassifier(criterion=\"entropy\", random_state=0), False),\n",
    "    \"Random\": (RandomForestClassifier(n_estimators=100, criterion=\"entropy\", random_state=0), False),\n",
    "}\n",
    "\n",
    "def make_pipeline_with_k(clf, k, std_after=True):\n",
    "    steps = [\n",
    "        (\"mm\", MinMaxScaler()),                # ensure non-negative for chi2\n",
    "        (\"kbest\", SelectKBest(score_func=chi2, k=k))\n",
    "    ]\n",
    "    if std_after:\n",
    "        steps.append((\"std\", StandardScaler()))\n",
    "    steps.append((\"clf\", clf))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "# Train/eval each model with its best k and print selected features + metrics\n",
    "results = []\n",
    "for name, (clf, std_after) in model_defs.items():\n",
    "    k = best_k_per_model.get(name)\n",
    "    pipe = make_pipeline_with_k(clf, k=k, std_after=std_after)\n",
    "\n",
    "    # Fit pipeline on training set (SelectKBest is fit on train inside pipeline)\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Extract selected feature names (based on columns in X_train)\n",
    "    selector = pipe.named_steps[\"kbest\"]\n",
    "    selected_mask = selector.get_support()\n",
    "    selected_features = X.columns[selected_mask].tolist()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Model: {name} (k={k})\")\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    print(\"Selected features:\", selected_features)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    results.append({\"model\": name, \"k\": k, \"test_acc\": acc})\n",
    "\n",
    "# summary table\n",
    "print(\"\\nSummary of test accuracies:\")\n",
    "print(pd.DataFrame(results).sort_values(\"test_acc\", ascending=False).reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf8e5d-879d-4234-abfe-329568958252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
